{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions on test dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading gradient classifier, topic model, and dimension reduction tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/lvasquez002/ixa/04/ixavenv2/lib/python3.6/site-packages/torch/cuda/__init__.py:80: UserWarning:\n",
      "\n",
      "CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# saving the classifier\n",
    "import os\n",
    "import pickle\n",
    "training_dir = \"training\"\n",
    "with open(os.path.join(training_dir, \"gradient_classifier.pk\"), \"rb\"\n",
    ") as f:\n",
    "    gradient_classifier = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(training_dir, \"topic_model.pk\"), \"rb\"\n",
    ") as f:\n",
    "    topic_model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(training_dir, \"pca_reduction.pk\"), \"rb\"\n",
    ") as f:\n",
    "    pca_reduction = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining classification for users\n",
    "\n",
    "Until now, we have a classifier for gradients. \n",
    "\n",
    "We have to use this classifier to assing a label to a user, given their documents.\n",
    "\n",
    "\n",
    "As we are doing early risk detection, note that false negatives are to be avoided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the algorithm for classifying users based on their documents:\n",
    "\n",
    "- The documents must be given in chronological order as a list\n",
    "- Using the topic model, we calculate the topic probabilities for each document\n",
    "- We calculate the logits and, using PCA, we reduce their dimension\n",
    "- We calculate the gradients as the difference of the logits of consecutive documents\n",
    "    - Given that calculating the gradients reduces the list length by 1, we add a dummy document (empty string) at the beginning of the document list.\n",
    "- Using the gradient classifier, we go through the gradients:\n",
    "    - On a gradient, calculate the probabilities that it corresponds to the positive or the negative label\n",
    "    - If one of the probabilities is above a predefined threshold (for example, 80%), assign the corresponding label to the user, and stop the classification. \n",
    "        - This is done because we focus on EARLY detection.\n",
    "    - If none of the probabilities is above the predefined threshold, move on to the next gradient\n",
    "    - Iterate until a label is assigned.\n",
    "    - If no label is assigned, label the user as at risk (assing the positive label)\n",
    "        - This is done to minimize false negatives  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.utils import basic_text_cleaning\n",
    "from myutils.utils import calculate_logits\n",
    "from myutils.utils import CLASSIFICATION_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification Threshold\n",
    "CLASSIFICATION_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 1, 'necessary_documents': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_based_on_documents(\n",
    "    document_list, # list of strings\n",
    "    topic_model, # gives probabilities to belong to a topic\n",
    "    dimensionality_reduction, # PCA or similar\n",
    "    gradient_classifier, # probabilistic classifier for the gradients\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the prediction for the user (positive, negative),\n",
    "    and also on which document the decision was made.\n",
    "    \n",
    "    For example: \n",
    "    (1, 3) means that the user was assigned label 1, \n",
    "    and that this decision was made after looking at document number 3\n",
    "    (using 1-indexing)\n",
    "    \"\"\"\n",
    "    # adding dummy document and cleaning the documents\n",
    "    document_list = [\"\"] + document_list\n",
    "    document_list = [basic_text_cleaning(doc) for doc in document_list]\n",
    "    n_docs = len(document_list)\n",
    "\n",
    "    # calculating logits\n",
    "    topics, probs = topic_model.transform(document_list)\n",
    "    user_logits = calculate_logits(probs)\n",
    "    \n",
    "    # calculating gradients and reducing dimensionality\n",
    "    user_gradients = [\n",
    "        list(next_logits - previous_logits)\n",
    "        for (previous_logits, next_logits)\n",
    "        in zip(\n",
    "            user_logits[0:n_docs-1], user_logits[1:n_docs]\n",
    "        )\n",
    "    ]\n",
    "    reduced_gradients = dimensionality_reduction.transform(user_gradients)\n",
    "\n",
    "    # go through the gradients one by one\n",
    "    prediction = None\n",
    "    necessary_documents = None\n",
    "\n",
    "    for document_index, red_gradient in enumerate(reduced_gradients):\n",
    "        # get the probabilities for labels 0 and 1\n",
    "        [[prob_neg, prob_pos]] = gradient_classifier.predict_proba(\n",
    "            X = [red_gradient]\n",
    "            )\n",
    "        # assing a category\n",
    "        if prob_neg >= CLASSIFICATION_THRESHOLD:\n",
    "            prediction = 0\n",
    "        if prob_pos >= CLASSIFICATION_THRESHOLD:\n",
    "            prediction = 1\n",
    "        if prediction is not None:\n",
    "            necessary_documents = document_index + 1 # 1-indexing\n",
    "            break\n",
    "    \n",
    "    # if no category is assigned, assing as \"at risk\"\n",
    "    if prediction is None:\n",
    "        prediction = 1\n",
    "        necessary_documents = len(reduced_gradients)\n",
    "\n",
    "    result = {\n",
    "        \"prediction\" : prediction,\n",
    "        \"necessary_documents\" : necessary_documents\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# a simple example\n",
    "classify_based_on_documents(\n",
    "    [\n",
    "        \"Hello, it is a good day outside\",\n",
    "        \"Today I want to eat pasta\",\n",
    "        \"Tell me what the problem is\",\n",
    "        \"I cannot wait to go to the park\",\n",
    "        \"Where is the nearest pizza place?\"\n",
    "    ],\n",
    "    topic_model=topic_model,\n",
    "    dimensionality_reduction=pca_reduction,\n",
    "    gradient_classifier=gradient_classifier\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "import json\n",
    "datasets_dir = \"datasets\"\n",
    "\n",
    "with open(os.path.join(datasets_dir, \"test_dataset.json\"), \"r\") as f:\n",
    "    test_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11959"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of users in test dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '00kate00',\n",
       " 'labeled_texts': [{'text': 'its been pouring with rain for 2 days maybe we wont have water restrictions after this rain i hope so',\n",
       "   'polarity': 1},\n",
       "  {'text': 'arrrrhhh, i did it again i past 100 and now 200 i was gunna say it was my 200th update but i didnt get to again',\n",
       "   'polarity': 0},\n",
       "  {'text': 'i wanna go see jason mraz in concert now after seeing him on rove but its sold out',\n",
       "   'polarity': 0},\n",
       "  {'text': 'thats was the fastest shower of my life, somebody kept turning on the water and it was going cold',\n",
       "   'polarity': 0},\n",
       "  {'text': 'nick is such a stud muffin', 'polarity': 1}],\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11959 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11959/11959 [7:28:05<00:00,  2.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Classifying users by their documents\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for entry in tqdm(test_dataset):\n",
    "    evaluation_entry = dict()\n",
    "\n",
    "    evaluation_entry[\"user\"] = entry[\"user\"]\n",
    "    evaluation_entry[\"true_label\"] = entry[\"label\"]\n",
    "\n",
    "    # get the documents\n",
    "    user_docs = [dic[\"text\"] for dic in entry[\"labeled_texts\"]]\n",
    "    evaluation_entry[\"total_documents\"] = len(user_docs)\n",
    "\n",
    "    # classify the user\n",
    "    classification_result = classify_based_on_documents(\n",
    "        user_docs,\n",
    "        topic_model=topic_model,\n",
    "        dimensionality_reduction=pca_reduction,\n",
    "        gradient_classifier=gradient_classifier\n",
    "    )\n",
    "\n",
    "    evaluation_entry[\"predicted_label\"] = classification_result[\"prediction\"]\n",
    "    evaluation_entry[\"necessary_documents\"] = classification_result[\"necessary_documents\"]\n",
    "\n",
    "    evaluation_results.append(evaluation_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '00kate00',\n",
       " 'true_label': 0,\n",
       " 'total_documents': 5,\n",
       " 'predicted_label': 1,\n",
       " 'necessary_documents': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '061004',\n",
       " 'true_label': 0,\n",
       " 'total_documents': 5,\n",
       " 'predicted_label': 0,\n",
       " 'necessary_documents': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dir = \"evaluation\"\n",
    "if not os.path.exists(evaluation_dir):\n",
    "    os.mkdir(evaluation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open(\n",
    "    os.path.join(evaluation_dir, \"classification_results.json\"),\n",
    "    \"w\"\n",
    ") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ixavenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
