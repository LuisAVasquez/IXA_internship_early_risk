{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_training_data = \"./datasets/full_dataset.csv\"\n",
    "os.path.exists(path_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    path_training_data, \n",
    "    header = None,\n",
    "    encoding = 'latin-1'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   query     1600000 non-null  object\n",
      " 4   user      1600000 non-null  object\n",
      " 5   text      1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"polarity\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates are imported as strings, we convert them to date objetcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date format to UTC\n",
    "dataset[\"date\"]=dataset[\"date\"].apply(lambda st: st.replace(\"PDT\", \"UTC-07:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"date\"] = pd.to_datetime(dataset[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"date\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by user and order by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_by_date(dataframe):\n",
    "\n",
    "    dataframe.sort_values(by=[\"date\"], ascending = True, inplace = True) \n",
    "    # dataframe = dataframe.drop([\"datetime\"], axis = 1)\n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the dataset.\n",
    "\n",
    "After inspection, the three top posters (tweetpet, webwoke, lost_dog), are bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "dancelikejordan      1\n",
       "havin_an_affair      1\n",
       "havicyeo             1\n",
       "haveyoumettony       1\n",
       "havetoexplode        1\n",
       "                  ... \n",
       "VioletsCRUK        279\n",
       "SallytheShizzle    281\n",
       "tweetpet           310\n",
       "webwoke            345\n",
       "lost_dog           549\n",
       "Length: 659775, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(\"user\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_tokens = 5\n",
    "min_n_docs = 5\n",
    "bot_list = [\"tweetpet\", \"lost_dog\", \"webwoke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/659775 [00:00<?, ?it/s]/gaueko0/users/lvasquez002/ixa/04/ixavenv2/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "100%|██████████| 659775/659775 [12:04<00:00, 910.39it/s]\n"
     ]
    }
   ],
   "source": [
    "final_dataset = []\n",
    "for user, sub_df in tqdm( dataset.groupby(\"user\")):\n",
    "    entry = {}\n",
    "    entry[\"user\"] = user\n",
    "    # keep only those texts long enough\n",
    "    tokens_list = list(sub_df[\"text\"].apply(str.split))\n",
    "    tokens_indexes = [len(tokens) >= min_n_tokens for tokens in tokens_list]\n",
    "    sub_df = sub_df[tokens_indexes] \n",
    "    # keep only those users with several texts\n",
    "    if len(sub_df) < min_n_docs:\n",
    "        continue\n",
    "    if user in bot_list:\n",
    "        continue\n",
    "    sub_df = reorder_by_date(sub_df)\n",
    "\n",
    "    texts = list(sub_df[\"text\"])\n",
    "    polarities = sub_df[\"polarity\"]\n",
    "    \n",
    "    # in the original dataset,\n",
    "    # 0 means negative, \n",
    "    # 4 means positive\n",
    "\n",
    "    polarities = [\n",
    "        1 if pol == 4 else 0 \n",
    "        for pol in sub_df[\"polarity\"]\n",
    "    ]\n",
    "    entry[\"labeled_texts\"] = [\n",
    "        {\n",
    "        \"text\":text,\n",
    "        \"polarity\":pol\n",
    "        }\n",
    "        for text, pol in zip(texts, polarities)\n",
    "    ]\n",
    "\n",
    "    final_dataset.append(entry)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to clean the text scrapped from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.utils import basic_text_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in final_dataset:\n",
    "\n",
    "    for text_entry in entry[\"labeled_texts\"]:\n",
    "        text_entry[\"text\"] = basic_text_cleaning(\n",
    "            text_entry[\"text\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we are trying to classify the users as in risk or not in risk, we need to assign categories to them in order to train a classifier later.\n",
    "That is, we need to have an annotated user dataset.\n",
    "\n",
    "To decide if a user is in risk or not, I categorise a user as in risk if at least 2/3rds of their tweets are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_for_negative_label = 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in final_dataset:\n",
    "    n_docs = len(entry[\"labeled_texts\"])\n",
    "    n_negative_docs = 0\n",
    "    for text_entry in entry[\"labeled_texts\"]:\n",
    "        if text_entry[\"polarity\"] == 0:\n",
    "            n_negative_docs +=1\n",
    "\n",
    "    if n_negative_docs >= proportion_for_negative_label * n_docs:\n",
    "        user_label = 1 # in risk\n",
    "    else:\n",
    "        user_label = 0 # not in risk\n",
    "    entry[\"label\"] = user_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    39778\n",
       "1    20016\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(final_dataset).groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '10isjess',\n",
       " 'labeled_texts': [{'text': 'totally jealous, i want to be at that party',\n",
       "   'polarity': 0},\n",
       "  {'text': 'feel better. i would offer to help but i am working on a presentation for tonight  on depression',\n",
       "   'polarity': 0},\n",
       "  {'text': 'went to run a 5k in honor of daniel wultz', 'polarity': 0},\n",
       "  {'text': \"don't want to be at work\", 'polarity': 0},\n",
       "  {'text': 'omg i have to write reports, only 11 more days till summer break',\n",
       "   'polarity': 1},\n",
       "  {'text': \"can't get any work done  to busy thinking about my boyfriend joey and how  we can suck the nectar together ;)\",\n",
       "   'polarity': 0},\n",
       "  {'text': 'serious withdrawals  thanks to those who posted pics they have been added to my phone',\n",
       "   'polarity': 0},\n",
       "  {'text': \"seriously is this day going in slow motion. ugh love pics but can't look anymore need to work\",\n",
       "   'polarity': 0},\n",
       "  {'text': 'oy my dad and i suffer from those. i feel for him. my dad just had one a couple of weeks ago. i hope he feels better soon',\n",
       "   'polarity': 0}],\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/filtered_dataset.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        final_dataset, f, indent=2\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [\n",
    "    entry[\"user\"] for entry in final_dataset\n",
    "]\n",
    "label_list = [\n",
    "    entry[\"label\"] for entry in final_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_train, users_test, labels_train, labels_test = train_test_split(\n",
    "    user_list, label_list,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [entry for entry in final_dataset if entry[\"user\"] \n",
    "                 in users_train\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [entry for entry in final_dataset if entry[\"user\"] \n",
    "                 in users_test\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/train_dataset.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        train_dataset, f, indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/test_dataset.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        test_dataset, f, indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47835"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling training dataset for experiments\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "downsample_factor = 0.05\n",
    "\n",
    "downsampled_train_dataset = random.sample(\n",
    "    train_dataset,\n",
    "    int( downsample_factor * len(train_dataset) ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/downsampled_train_dataset.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        downsampled_train_dataset, f, indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ixavenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
