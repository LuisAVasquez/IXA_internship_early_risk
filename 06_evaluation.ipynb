{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the user classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $F_{latency}$, as defined in Parapar, Losada 2022 (Overview of eRisk at CLEF 2021: Early Risk Prediction on the Internet (Extended Overview))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "evaluation_dir = \"evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(evaluation_dir, \"classification_results.json\"),\n",
    "    \"r\"\n",
    ") as f:\n",
    "    classification_results = json.load( f,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '00kate00',\n",
       " 'true_label': 0,\n",
       " 'total_documents': 5,\n",
       " 'predicted_label': 1,\n",
       " 'necessary_documents': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_results[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining F_latency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the penalty and the speed. \n",
    "Calculating the penalty requires a hyperparameter $p$, which the authors set to $0.0078$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0078"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myutils.utils import P_PARAMETER\n",
    "P_PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty depends only on the number of documents seen before taking a decision.\n",
    "# Calculating the penalty requires 1-indexing!\n",
    "def calculate_penalty(n_documents_seen):\n",
    "    exponent = -P_PARAMETER*(n_documents_seen-1)\n",
    "    return -1 + 2/(1 + np.exp(exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_penalty(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed is calculated only considering the true positives\n",
    "def calculate_speed(\n",
    "    classification_results # dictionary containing true and predicted labels, and documents required for the decision\n",
    "):\n",
    "    true_positives = []\n",
    "    for entry in classification_results:\n",
    "        if entry[\"true_label\"] ==1 and entry[\"predicted_label\"]==1:\n",
    "            true_positives.append(entry)\n",
    "\n",
    "    # calculate penalties for the true positives\n",
    "    tp_penalties = []\n",
    "    for entry in true_positives:\n",
    "        penalty = calculate_penalty(\n",
    "            # penalties require 1-indexing \n",
    "            # Being conscious of this, we saved the evaluation results with 1-indexing\n",
    "            entry[\"necessary_documents\"]\n",
    "        )\n",
    "        tp_penalties.append(penalty)\n",
    "    \n",
    "    # calculate speed\n",
    "    median = np.median(tp_penalties)\n",
    "    print(f\"Median penalties for TPs: {median}\")\n",
    "    speed = 1 - median\n",
    "\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median penalties for TPs: 0.019497528750876292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9805024712491237"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_speed(classification_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of this classifier is really high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_{latency} = F_1 * speed$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F_1\n",
    "def calculate_f_1(\n",
    "        classification_results\n",
    "):\n",
    "    true_labels = [entry[\"true_label\"] for entry in classification_results]\n",
    "    predicted_labels = [entry[\"predicted_label\"] for entry in classification_results]\n",
    "\n",
    "    f_1 = f1_score(y_true=true_labels, y_pred=predicted_labels)\n",
    "    return f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3603805260212647"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f_1(classification_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F_1$ is low. Remember that for this test we're training with a downsampled training dataset, due to computational limitations.\n",
    "With the complete training dataset, the $F_1$ should improve significatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define F-latency\n",
    "def calculate_f_latency(\n",
    "        classification_results\n",
    "):\n",
    "    f_1 = calculate_f_1(classification_results)\n",
    "    speed = calculate_speed(classification_results)\n",
    "\n",
    "    f_latency = f_1 * speed\n",
    "    return f_latency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $F_{latency}$ for our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median penalties for TPs: 0.019497528750876292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3533539963539092"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f_latency(classification_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to interpret this ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is quick to take decisions for true positives, but in general its decisions are not reliable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training with the complete training set\n",
    "- Playing with the hyperparameter\n",
    "- Trying different dimensionality reduction (PCA, UMAP, LDA, etc) and classification methods (Logistic Regression, GB classifiers, etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ixavenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
